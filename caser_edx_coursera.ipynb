{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# Assuming these are custom modules you have in your project\n",
    "from interactions import Interactions\n",
    "from evaluation import evaluate_ranking\n",
    "from utils import set_seed, shuffle, str2bool\n",
    "from caser import Caser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(object):\n",
    "    def __init__(self, n_iter, batch_size, learning_rate, l2, neg_samples, model_args, use_cuda, precomputed_embeddings):\n",
    "        self._num_items = None\n",
    "        self._num_users = None\n",
    "        self._net = None\n",
    "        self.model_args = model_args\n",
    "        self._n_iter = n_iter\n",
    "        self._batch_size = batch_size\n",
    "        self._learning_rate = learning_rate\n",
    "        self._l2 = l2\n",
    "        self._neg_samples = neg_samples\n",
    "        self._device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "        self.precomputed_embeddings = torch.from_numpy(precomputed_embeddings).float().to(self._device)\n",
    "        self._initialized = False\n",
    "\n",
    "    def _initialize(self, interactions):\n",
    "        self._num_items = interactions.num_items\n",
    "        self._num_users = interactions.num_users\n",
    "        self.test_sequence = interactions.test_sequences\n",
    "\n",
    "        # Assume the model_args have the necessary parameters\n",
    "        self._net = Caser(self._num_users, self._num_items, self.model_args).to(self._device)\n",
    "\n",
    "        self._optimizer = optim.Adam(self._net.parameters(), weight_decay=self._l2, lr=self._learning_rate)\n",
    "        self._initialized = True\n",
    "\n",
    "    def fit(self, train, test, verbose=False):\n",
    "        if not self._initialized:\n",
    "            self._initialize(train)\n",
    "\n",
    "        # Convert to sequences, targets, and users\n",
    "        sequences_np = train.sequences.sequences\n",
    "        targets_np = train.sequences.targets\n",
    "        users_np = train.sequences.user_ids.reshape(-1, 1)\n",
    "\n",
    "        L, T = train.sequences.L, train.sequences.T\n",
    "\n",
    "        n_train = sequences_np.shape[0]\n",
    "\n",
    "        print(f'Total training instances: {n_train}')\n",
    "\n",
    "        for epoch_num in range(self._n_iter):\n",
    "            t1 = time()\n",
    "\n",
    "            # Set model to training mode\n",
    "            self._net.train()\n",
    "\n",
    "            users_np, sequences_np, targets_np = shuffle(users_np, sequences_np, targets_np)\n",
    "\n",
    "            negatives_np = self._generate_negative_samples(users_np, train, n=self._neg_samples)\n",
    "\n",
    "            # Convert numpy arrays to PyTorch tensors and move them to the corresponding devices\n",
    "            users = torch.from_numpy(users_np).long().to(self._device)\n",
    "            sequences = torch.from_numpy(sequences_np).long().to(self._device)\n",
    "            targets = torch.from_numpy(targets_np).long().to(self._device)\n",
    "            negatives = torch.from_numpy(negatives_np).long().to(self._device)\n",
    "\n",
    "            items_to_predict = torch.cat((targets, negatives), 1)\n",
    "            items_prediction = self._net(sequences, users, items_to_predict)\n",
    "\n",
    "            targets_prediction = items_prediction[:, :targets.size(1)]\n",
    "            negatives_prediction = items_prediction[:, targets.size(1):]\n",
    "\n",
    "            # Compute the binary cross-entropy loss\n",
    "            positive_loss = -torch.mean(torch.log(torch.sigmoid(targets_prediction) + 1e-8))\n",
    "            negative_loss = -torch.mean(torch.log(1 - torch.sigmoid(negatives_prediction) + 1e-8))\n",
    "            loss = positive_loss + negative_loss\n",
    "\n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "            self._optimizer.zero_grad()\n",
    "\n",
    "            t2 = time()\n",
    "\n",
    "            print(f\"Epoch {epoch_num + 1}/{self._n_iter} [{t2 - t1:.2f}s]\\tloss={loss.item():.4f}\")\n",
    "\n",
    "    def _generate_negative_samples(self, users, interactions, n):\n",
    "        \"\"\"\n",
    "        Sample negative items for each user.\n",
    "        \"\"\"\n",
    "        users_ = users.squeeze()\n",
    "        negative_samples = np.zeros((users_.shape[0], n), np.int64)\n",
    "        if not hasattr(self, '_candidate'):\n",
    "            self._candidate = {}\n",
    "            for u, row in enumerate(interactions.tocsr()):\n",
    "                self._candidate[u] = list(set(np.arange(interactions.num_items)) - set(row.indices))\n",
    "\n",
    "        for i, u in enumerate(users_):\n",
    "            for j in range(n):\n",
    "                x = self._candidate[u]\n",
    "                negative_samples[i, j] = x[np.random.randint(len(x))]\n",
    "\n",
    "        return negative_samples\n",
    "\n",
    "    def predict(self, user_id, item_ids=None):\n",
    "        \"\"\"\n",
    "        Make predictions for evaluation: given a user id, it will\n",
    "        first retrieve the test sequence associated with that user\n",
    "        and compute the recommendation scores for items.\n",
    "        \"\"\"\n",
    "        if self.test_sequence is None:\n",
    "            raise ValueError('Missing test sequences, cannot make predictions')\n",
    "\n",
    "        self._net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sequences_np = self.test_sequence.sequences[user_id, :]\n",
    "            sequences_np = np.atleast_2d(sequences_np)\n",
    "\n",
    "            if item_ids is None:\n",
    "                item_ids = np.arange(self._num_items).reshape(-1)\n",
    "\n",
    "            sequences = torch.from_numpy(sequences_np).long().to(self._device)\n",
    "            item_ids = torch.from_numpy(item_ids).long().to(self._device)\n",
    "            user_id = torch.from_numpy(np.array([[user_id]])).long().to(self._device)\n",
    "\n",
    "            # Reshape inputs to match the expected shapes\n",
    "            sequences = sequences.unsqueeze(0)  # Add batch dimension\n",
    "            user_id = user_id.squeeze(1)  # Remove unnecessary dimension\n",
    "\n",
    "            out = self._net(sequences, user_id, item_ids, for_pred=True)\n",
    "\n",
    "        return out.cpu().numpy().flatten()\n",
    "\n",
    "    def load_pretrained_model(self, path):\n",
    "        if self._net is None:\n",
    "            raise ValueError(\"Model is not initialized. Please call the _initialize method first.\")\n",
    "        \n",
    "        pretrained_dict = torch.load(path, map_location=self._device)\n",
    "        model_dict = self._net.state_dict()\n",
    "\n",
    "        # Filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "\n",
    "        # Overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "\n",
    "        # Load the new state dict into the model\n",
    "        self._net.load_state_dict(model_dict)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self._net.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, test_data, config, pretrained_model_path=None, is_pretrain=True):\n",
    "    print(f\"{'Pretraining' if is_pretrain else 'Fine-tuning'} the model...\")\n",
    "    print(f\"Number of users: {train_data.num_users}\")\n",
    "    print(f\"Number of items: {train_data.num_items}\")\n",
    "    print(f\"Number of interactions: {len(train_data.sequences.sequences)}\")\n",
    "    \n",
    "    # Initialize the model before loading\n",
    "    if not model._initialized:\n",
    "        model._initialize(train_data)\n",
    "\n",
    "    if not is_pretrain and pretrained_model_path:\n",
    "        model.load_pretrained_model(pretrained_model_path)\n",
    "    \n",
    "    model.fit(train_data, test_data, verbose=True)\n",
    "    \n",
    "    if is_pretrain:\n",
    "        model.save_model('edx_pretrained_model.pth')\n",
    "    else:\n",
    "        model.save_model('coursera_finetuned_model.pth')\n",
    "\n",
    "    # Perform final evaluation\n",
    "    print(\"Performing final evaluation...\")\n",
    "    precision, recall, mean_aps, mrr, ndcg = evaluate_ranking(model, test_data, train_data, k=[1, 5, 10])\n",
    "    print(f\"Final results:\")\n",
    "    print(f\"Precision: @1={precision[0].mean():.4f}, @5={precision[1].mean():.4f}, @10={precision[2].mean():.4f}\")\n",
    "    print(f\"Recall: @1={recall[0].mean():.4f}, @5={recall[1].mean():.4f}, @10={recall[2].mean():.4f}\")\n",
    "    print(f\"MAP={mean_aps:.4f}, MRR={mrr:.4f}, NDCG={ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "edx_train_root: datasets/edx/train.txt\n",
      "edx_test_root: datasets/edx/test.txt\n",
      "coursera_train_root: datasets/coursera/train.txt\n",
      "coursera_test_root: datasets/coursera/test.txt\n",
      "L: 5\n",
      "T: 3\n",
      "n_iter: 20\n",
      "seed: 1234\n",
      "batch_size: 512\n",
      "learning_rate: 0.001\n",
      "l2: 1e-06\n",
      "neg_samples: 3\n",
      "use_cuda: True\n",
      "\n",
      "Model configuration:\n",
      "d: 512\n",
      "nv: 4\n",
      "nh: 16\n",
      "drop: 0.5\n",
      "ac_conv: relu\n",
      "ac_fc: relu\n",
      "L: 5\n"
     ]
    }
   ],
   "source": [
    "# Data arguments\n",
    "edx_train_root = 'datasets/edx/train.txt'\n",
    "edx_test_root = 'datasets/edx/test.txt'\n",
    "coursera_train_root = 'datasets/coursera/train.txt'\n",
    "coursera_test_root = 'datasets/coursera/test.txt'\n",
    "L = 5\n",
    "T = 3\n",
    "\n",
    "# Train arguments\n",
    "n_iter = 20\n",
    "seed = 1234\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "l2 = 1e-6\n",
    "neg_samples = 3\n",
    "use_cuda = True  # ให้แน่ใจว่าคุณมี GPU พร้อมใช้งาน ถ้าไม่มีให้ตั้งค่าเป็น False\n",
    "\n",
    "# Model arguments\n",
    "d = 512\n",
    "nv = 4\n",
    "nh = 16\n",
    "drop = 0.5\n",
    "ac_conv = 'relu'\n",
    "ac_fc = 'relu'\n",
    "\n",
    "# สร้าง config object เพื่อให้สามารถใช้งานได้เหมือนเดิม\n",
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "config = Config(\n",
    "    edx_train_root=edx_train_root,\n",
    "    edx_test_root=edx_test_root,\n",
    "    coursera_train_root=coursera_train_root,\n",
    "    coursera_test_root=coursera_test_root,\n",
    "    L=L,\n",
    "    T=T,\n",
    "    n_iter=n_iter,\n",
    "    seed=seed,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    l2=l2,\n",
    "    neg_samples=neg_samples,\n",
    "    use_cuda=use_cuda\n",
    ")\n",
    "\n",
    "model_config = Config(\n",
    "    d=d,\n",
    "    nv=nv,\n",
    "    nh=nh,\n",
    "    drop=drop,\n",
    "    ac_conv=ac_conv,\n",
    "    ac_fc=ac_fc,\n",
    "    L=L\n",
    ")\n",
    "\n",
    "# แสดงค่า config เพื่อตรวจสอบ\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.__dict__.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nModel configuration:\")\n",
    "for key, value in model_config.__dict__.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences after filtering: 4469\n",
      "item_map {'0': 0, '1': 1, '6': 2, '2': 3, '8': 4, '13': 5, '5': 6, '3': 7, '11': 8, '15': 9, '10': 10, '9': 11, '4': 12, '7': 13, '14': 14, '12': 15}\n",
      "Total sequences after filtering: 168\n",
      "item_map {'224': 0, '183': 1, '225': 2, '127': 3, '58': 4, '221': 5, '169': 6, '120': 7, '121': 8, '15': 9, '135': 10, '255': 11, '258': 12, '100': 13, '11': 14, '0': 15, '162': 16, '256': 17, '118': 18, '266': 19, '267': 20, '265': 21, '7': 22, '39': 23, '52': 24, '27': 25, '57': 26, '274': 27, '304': 28, '214': 29, '16': 30, '97': 31, '43': 32, '95': 33, '294': 34, '308': 35, '136': 36, '20': 37, '170': 38, '3': 39, '145': 40, '59': 41, '142': 42, '4': 43, '107': 44, '1': 45, '299': 46, '326': 47, '186': 48, '47': 49, '250': 50, '87': 51, '5': 52, '166': 53, '175': 54, '105': 55, '330': 56, '204': 57, '309': 58, '101': 59, '296': 60, '227': 61, '226': 62, '228': 63, '88': 64, '8': 65, '273': 66, '178': 67, '132': 68, '192': 69, '229': 70, '319': 71, '248': 72, '23': 73, '115': 74, '252': 75, '249': 76, '70': 77, '71': 78, '261': 79, '62': 80, '289': 81, '163': 82, '301': 83, '297': 84, '240': 85, '21': 86, '262': 87, '38': 88, '75': 89, '251': 90, '328': 91, '264': 92, '124': 93, '28': 94, '181': 95, '205': 96, '280': 97, '333': 98, '318': 99, '278': 100, '232': 101, '117': 102, '316': 103, '69': 104, '72': 105, '257': 106, '165': 107, '179': 108, '322': 109, '42': 110, '206': 111, '76': 112, '55': 113, '300': 114, '93': 115, '74': 116, '61': 117, '81': 118, '325': 119, '284': 120, '77': 121, '212': 122, '151': 123, '94': 124, '138': 125, '303': 126, '102': 127, '91': 128, '148': 129, '36': 130, '83': 131, '126': 132, '45': 133, '9': 134, '10': 135, '78': 136, '50': 137, '253': 138, '291': 139, '68': 140, '222': 141, '197': 142, '167': 143, '305': 144, '209': 145, '31': 146, '208': 147, '200': 148, '2': 149, '134': 150, '172': 151, '292': 152, '56': 153, '49': 154, '282': 155, '193': 156, '104': 157, '103': 158, '116': 159, '128': 160, '32': 161, '263': 162, '34': 163, '210': 164, '195': 165, '177': 166, '199': 167, '92': 168, '33': 169, '323': 170, '37': 171, '201': 172, '159': 173, '188': 174, '182': 175, '65': 176, '236': 177, '327': 178, '6': 179, '203': 180, '112': 181, '268': 182, '211': 183, '113': 184, '86': 185, '85': 186, '213': 187, '194': 188, '125': 189, '279': 190, '17': 191, '223': 192, '187': 193, '276': 194, '281': 195, '259': 196, '30': 197, '302': 198, '295': 199, '260': 200, '298': 201, '35': 202, '306': 203, '63': 204, '219': 205, '96': 206, '108': 207, '131': 208, '331': 209, '84': 210, '154': 211, '320': 212, '290': 213, '110': 214, '174': 215, '233': 216, '54': 217, '53': 218, '89': 219, '155': 220, '156': 221, '217': 222, '82': 223, '234': 224, '235': 225, '122': 226, '190': 227, '191': 228, '314': 229, '287': 230, '129': 231, '173': 232, '313': 233, '119': 234, '207': 235, '329': 236, '180': 237, '160': 238, '109': 239, '315': 240, '168': 241, '230': 242, '241': 243, '288': 244, '254': 245, '26': 246, '106': 247, '176': 248, '143': 249, '73': 250, '79': 251, '286': 252, '152': 253, '324': 254, '146': 255, '277': 256, '196': 257, '147': 258, '198': 259, '238': 260, '123': 261, '44': 262, '312': 263, '19': 264, '275': 265, '245': 266, '246': 267, '218': 268, '220': 269, '185': 270, '242': 271, '310': 272, '237': 273, '321': 274, '307': 275, '12': 276, '332': 277, '40': 278, '29': 279, '161': 280, '244': 281, '41': 282, '171': 283, '90': 284}\n",
      "Configuration:\n",
      "edx_train_root: datasets/edx/train.txt\n",
      "edx_test_root: datasets/edx/test.txt\n",
      "coursera_train_root: datasets/coursera/train.txt\n",
      "coursera_test_root: datasets/coursera/test.txt\n",
      "L: 5\n",
      "T: 3\n",
      "n_iter: 20\n",
      "seed: 1234\n",
      "batch_size: 512\n",
      "learning_rate: 0.001\n",
      "l2: 1e-06\n",
      "neg_samples: 3\n",
      "use_cuda: True\n",
      "\n",
      "Model configuration:\n",
      "d: 512\n",
      "nv: 4\n",
      "nh: 16\n",
      "drop: 0.5\n",
      "ac_conv: relu\n",
      "ac_fc: relu\n",
      "L: 5\n",
      "Pretraining the model...\n",
      "Number of users: 359439\n",
      "Number of items: 16\n",
      "Number of interactions: 4469\n",
      "Total training instances: 4469\n",
      "Epoch 1/20 [7.19s]\tloss=1.3868\n",
      "Epoch 2/20 [0.06s]\tloss=1.3496\n",
      "Epoch 3/20 [0.06s]\tloss=1.2973\n",
      "Epoch 4/20 [0.06s]\tloss=1.2776\n",
      "Epoch 5/20 [0.06s]\tloss=1.2868\n",
      "Epoch 6/20 [0.05s]\tloss=1.2619\n",
      "Epoch 7/20 [0.06s]\tloss=1.2451\n",
      "Epoch 8/20 [0.04s]\tloss=1.2416\n",
      "Epoch 9/20 [0.05s]\tloss=1.2439\n",
      "Epoch 10/20 [0.07s]\tloss=1.2368\n",
      "Epoch 11/20 [0.06s]\tloss=1.2254\n",
      "Epoch 12/20 [0.07s]\tloss=1.2204\n",
      "Epoch 13/20 [0.07s]\tloss=1.2080\n",
      "Epoch 14/20 [0.06s]\tloss=1.2001\n",
      "Epoch 15/20 [0.06s]\tloss=1.1874\n",
      "Epoch 16/20 [0.07s]\tloss=1.1828\n",
      "Epoch 17/20 [0.07s]\tloss=1.1683\n",
      "Epoch 18/20 [0.06s]\tloss=1.1688\n",
      "Epoch 19/20 [0.07s]\tloss=1.1538\n",
      "Epoch 20/20 [0.06s]\tloss=1.1458\n",
      "Model saved to edx_pretrained_model.pth\n",
      "Performing final evaluation...\n",
      "Processed 30299 valid users out of 420084 total users\n",
      "Final results:\n",
      "Precision: @1=0.3156, @5=0.1620, @10=0.1008\n",
      "Recall: @1=0.2925, @5=0.7383, @10=0.9145\n",
      "MAP=0.5029, MRR=0.5052, NDCG=0.9207\n",
      "Fine-tuning the model...\n",
      "Number of users: 465\n",
      "Number of items: 285\n",
      "Number of interactions: 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMPUTER15\\AppData\\Local\\Temp\\ipykernel_7112\\249531272.py:129: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(path, map_location=self._device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training instances: 168\n",
      "Epoch 1/20 [0.03s]\tloss=1.3861\n",
      "Epoch 2/20 [0.00s]\tloss=1.3394\n",
      "Epoch 3/20 [0.00s]\tloss=1.2615\n",
      "Epoch 4/20 [0.00s]\tloss=1.1680\n",
      "Epoch 5/20 [0.01s]\tloss=1.0411\n",
      "Epoch 6/20 [0.00s]\tloss=0.9775\n",
      "Epoch 7/20 [0.01s]\tloss=0.9432\n",
      "Epoch 8/20 [0.00s]\tloss=0.9226\n",
      "Epoch 9/20 [0.01s]\tloss=0.7992\n",
      "Epoch 10/20 [0.00s]\tloss=0.7961\n",
      "Epoch 11/20 [0.00s]\tloss=0.8621\n",
      "Epoch 12/20 [0.01s]\tloss=0.7270\n",
      "Epoch 13/20 [0.00s]\tloss=0.7398\n",
      "Epoch 14/20 [0.01s]\tloss=0.6914\n",
      "Epoch 15/20 [0.00s]\tloss=0.7220\n",
      "Epoch 16/20 [0.01s]\tloss=0.6643\n",
      "Epoch 17/20 [0.00s]\tloss=0.6123\n",
      "Epoch 18/20 [0.00s]\tloss=0.6024\n",
      "Epoch 19/20 [0.02s]\tloss=0.5647\n",
      "Epoch 20/20 [0.00s]\tloss=0.5821\n",
      "Model saved to coursera_finetuned_model.pth\n",
      "Performing final evaluation...\n",
      "Processed 465 valid users out of 465 total users\n",
      "Final results:\n",
      "Precision: @1=0.0108, @5=0.0159, @10=0.0146\n",
      "Recall: @1=0.0065, @5=0.0699, @10=0.1319\n",
      "MAP=0.0403, MRR=0.0381, NDCG=0.1355\n",
      "Pretraining and fine-tuning completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed(seed, use_cuda)\n",
    "\n",
    "# Load edX dataset\n",
    "edx_train = Interactions(edx_train_root)\n",
    "edx_train.to_sequence(L, T)\n",
    "edx_test = Interactions(edx_test_root, user_map=edx_train.user_map, item_map=edx_train.item_map)\n",
    "\n",
    "# Load Coursera dataset\n",
    "coursera_train = Interactions(coursera_train_root)\n",
    "coursera_train.to_sequence(L, T)\n",
    "coursera_test = Interactions(coursera_test_root, user_map=coursera_train.user_map, item_map=coursera_train.item_map)\n",
    "\n",
    "# Load precomputed embeddings for edX\n",
    "edx_precomputed_embeddings = np.load(\"datasets/edx/precomputed_embeddings.npy\")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.__dict__.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"\\nModel configuration:\")\n",
    "for key, value in model_config.__dict__.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Create and pretrain the model on edX data\n",
    "edx_model = Recommender(n_iter=n_iter,\n",
    "                        batch_size=batch_size,\n",
    "                        learning_rate=learning_rate,\n",
    "                        l2=l2,\n",
    "                        neg_samples=neg_samples,\n",
    "                        model_args=model_config,\n",
    "                        use_cuda=use_cuda,\n",
    "                        precomputed_embeddings=edx_precomputed_embeddings)\n",
    "\n",
    "train_model(edx_model, edx_train, edx_test, config, is_pretrain=True)\n",
    "\n",
    "# Load precomputed embeddings for Coursera\n",
    "coursera_precomputed_embeddings = np.load(\"datasets/coursera/precomputed_embeddings.npy\")\n",
    "\n",
    "# Create a new model for Coursera, initialize with pretrained weights\n",
    "coursera_model = Recommender(n_iter=n_iter,\n",
    "                                batch_size=batch_size,\n",
    "                                learning_rate=learning_rate,\n",
    "                                l2=l2,\n",
    "                                neg_samples=neg_samples,\n",
    "                                model_args=model_config,\n",
    "                                use_cuda=use_cuda,\n",
    "                                precomputed_embeddings=coursera_precomputed_embeddings)\n",
    "\n",
    "# Fine-tune on Coursera data\n",
    "train_model(coursera_model, coursera_train, coursera_test, config, pretrained_model_path='edx_pretrained_model.pth', is_pretrain=False)\n",
    "\n",
    "print(\"Pretraining and fine-tuning completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caser_venv_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
