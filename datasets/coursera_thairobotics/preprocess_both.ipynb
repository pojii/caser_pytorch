{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Email               Course Name  \\\n",
      "0                 user1@anon.com    Support Vector Machine   \n",
      "1                 user2@anon.com    Support Vector Machine   \n",
      "2                 user3@anon.com    Support Vector Machine   \n",
      "3                 user4@anon.com    Support Vector Machine   \n",
      "4                 user5@anon.com  Dimensionality Reduction   \n",
      "...                          ...                       ...   \n",
      "2753     nutcha.tavo@kmutt.ac.th       Material Processing   \n",
      "2754  wannaporn.ruam@kmutt.ac.th      Ferrous Technology I   \n",
      "2755  wannaporn.ruam@kmutt.ac.th      Ferrous Technology I   \n",
      "2756  wannaporn.ruam@kmutt.ac.th      Ferrous Technology I   \n",
      "2757  wannaporn.ruam@kmutt.ac.th      Ferrous Technology I   \n",
      "\n",
      "                              Category  \n",
      "0                  Computer Science AI  \n",
      "1                  Computer Science AI  \n",
      "2                  Computer Science AI  \n",
      "3                  Computer Science AI  \n",
      "4                  Computer Science AI  \n",
      "...                                ...  \n",
      "2753  physical-science-and-engineering  \n",
      "2754  physical-science-and-engineering  \n",
      "2755  physical-science-and-engineering  \n",
      "2756  physical-science-and-engineering  \n",
      "2757  physical-science-and-engineering  \n",
      "\n",
      "[21958 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "df1 = pd.read_csv('../thairobotics/anonymized_combined_output_with_categories.csv')\n",
    "df2 = pd.read_csv('../coursera/filtered_usage_course.csv')\n",
    "\n",
    "# Select relevant columns from df1\n",
    "df1_selected = df1[['Email', 'Course Name', 'Category']]\n",
    "\n",
    "# Select relevant columns from df2 and rename them to match df1\n",
    "df2_selected = df2[['Email', 'Course', 'Domain']]\n",
    "df2_selected.columns = ['Email', 'Course Name', 'Category']\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df_combined = pd.concat([df1_selected, df2_selected])\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('combined_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved as train.txt, val.txt, test.txt, and text.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Load the anonymized CSV file\n",
    "df = pd.read_csv('combined_output.csv')\n",
    "\n",
    "# Step 1: Create a mapping from course name to a unique ID (starting from 1 for text.txt)\n",
    "course_name_map = {name: idx + 1 for idx, name in enumerate(df['Course Name'].unique())}\n",
    "\n",
    "# Step 2: Apply the mapping to the DataFrame\n",
    "df['course_id_mapped'] = df['Course Name'].map(course_name_map)\n",
    "\n",
    "# Step 3: Create user mappings to unique IDs (starting from 1 for consistency in train/val/test split)\n",
    "user_name_map = {name: idx + 1 for idx, name in enumerate(df['Email'].unique())}\n",
    "df['user_id_mapped'] = df['Email'].map(user_name_map)\n",
    "\n",
    "# Step 4: Keep only the relevant columns\n",
    "df = df[['user_id_mapped', 'course_id_mapped']]\n",
    "\n",
    "# Step 5: Split the data into train, val, and test sets using GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Ensure users are not split between train/val/test\n",
    "train_idx, temp_idx = next(gss.split(df, groups=df['user_id_mapped']))\n",
    "train = df.iloc[train_idx]\n",
    "temp = df.iloc[temp_idx]\n",
    "\n",
    "# Split the temp set into validation and test sets, ensuring users are not split\n",
    "gss_val_test = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx, test_idx = next(gss_val_test.split(temp, groups=temp['user_id_mapped']))\n",
    "val = temp.iloc[val_idx]\n",
    "test = temp.iloc[test_idx]\n",
    "\n",
    "# Step 6: Save the train, val, and test sets as .txt files\n",
    "train.to_csv('train.txt', sep=' ', header=False, index=False)\n",
    "val.to_csv('val.txt', sep=' ', header=False, index=False)\n",
    "test.to_csv('test.txt', sep=' ', header=False, index=False)\n",
    "\n",
    "# Step 7: Save the course name mapping as text.txt\n",
    "with open('text.txt', 'w', encoding='utf-8') as f:\n",
    "    for course_name, course_id in course_name_map.items():\n",
    "        f.write(f\"{course_id} {course_name}\\n\")\n",
    "\n",
    "print(\"Files saved as train.txt, val.txt, test.txt, and text.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved as course_codes.npy and course_embeddings.npy.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text  # ต้องเพิ่มเพื่อรองรับ SentencepieceOp\n",
    "\n",
    "# โหลดโมเดล MUSE จาก TensorFlow Hub\n",
    "muse_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "\n",
    "# โหลดไฟล์ text.txt\n",
    "course_codes = []\n",
    "course_names = []\n",
    "\n",
    "with open('text.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ', 1)  # แบ่งเป็น course_id กับ course_name\n",
    "        if len(parts) == 2:\n",
    "            course_id, course_name = parts\n",
    "            course_codes.append(int(course_id) + 1)  # เพิ่ม 1 ให้ index เริ่มจาก 1\n",
    "            course_names.append(course_name)\n",
    "\n",
    "# สร้าง embeddings จาก MUSE สำหรับชื่อคอร์ส\n",
    "course_embeddings = muse_model(course_names).numpy()\n",
    "\n",
    "# บันทึกคอร์สโค้ดเป็นไฟล์ .npy\n",
    "np.save('course_codes.npy', np.array(course_codes))\n",
    "\n",
    "# บันทึก embeddings เป็นไฟล์ .npy\n",
    "np.save('course_embeddings.npy', course_embeddings)\n",
    "\n",
    "print(\"Files saved as course_codes.npy and course_embeddings.npy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved as 'course_codes.npy' and 'precomputed_embeddings.npy'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text  # ต้องเพิ่มเพื่อรองรับ SentencepieceOp\n",
    "\n",
    "# โหลดโมเดล MUSE จาก TensorFlow Hub\n",
    "muse_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "\n",
    "# โหลดไฟล์ text.txt สำหรับ coursera_thairobotics\n",
    "course_codes = []\n",
    "course_names = []\n",
    "\n",
    "with open('text.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ', 1)  # แบ่งเป็น course_id กับ course_name\n",
    "        if len(parts) == 2:\n",
    "            course_id, course_name = parts\n",
    "            course_codes.append(int(course_id) + 1)  # เพิ่ม 1 เพื่อให้ index เริ่มจาก 1\n",
    "            course_names.append(course_name)\n",
    "\n",
    "# สร้าง embeddings จาก MUSE สำหรับชื่อคอร์ส\n",
    "course_embeddings = muse_model(course_names).numpy()\n",
    "\n",
    "# บันทึกคอร์สโค้ดเป็นไฟล์ .npy\n",
    "np.save('course_codes.npy', np.array(course_codes))\n",
    "\n",
    "# บันทึก embeddings เป็นไฟล์ .npy\n",
    "np.save('precomputed_embeddings.npy', course_embeddings)\n",
    "\n",
    "print(\"Files saved as 'course_codes.npy' and 'precomputed_embeddings.npy'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caser_venv_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
