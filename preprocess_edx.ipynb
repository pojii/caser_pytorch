{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\computer15\\desktop\\caser_pytorch\\caser_venv_py39\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\computer15\\desktop\\caser_pytorch\\caser_venv_py39\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.1-cp39-cp39-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/11.0 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/11.0 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.6/11.0 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.0/11.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 11.8 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "usage_report = pd.read_csv(r'C:\\Users\\COMPUTER15\\Desktop\\caser_pytorch\\datasets\\edx\\HMXPC13_DI_v2_5-14-14.csv')\n",
    "embed_df = pd.read_csv(r'C:\\Users\\COMPUTER15\\Desktop\\caser_pytorch\\datasets\\edx\\course_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(usage_report, embed_df, on='course_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Files saved as train.txt, val.txt, test.txt, and text.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "edx_data = pd.read_csv('datasets/edx/HMXPC13_DI_v2_5-14-14.csv')\n",
    "course_embeddings = pd.read_csv('datasets/edx/course_embeddings.csv')\n",
    "\n",
    "# Create a mapping from course_id to a unique integer\n",
    "course_id_map = {cid: idx for idx, cid in enumerate(course_embeddings['course_id'].unique())}\n",
    "course_name_map = {idx: name for idx, name in enumerate(course_embeddings['course_name'])}\n",
    "\n",
    "# Create a mapping from user_id to a unique integer\n",
    "user_id_map = {uid: idx for idx, uid in enumerate(edx_data['userid_DI'].unique())}\n",
    "\n",
    "# Apply the mappings to the data, using a default value for unmapped entries\n",
    "edx_data['user_id_mapped'] = edx_data['userid_DI'].map(user_id_map)\n",
    "edx_data['course_id_mapped'] = edx_data['course_id'].map(course_id_map)\n",
    "\n",
    "# Drop rows with NaN values in the mapped columns (if mapping fails)\n",
    "edx_data.dropna(subset=['user_id_mapped', 'course_id_mapped'], inplace=True)\n",
    "\n",
    "# Only keep relevant columns\n",
    "edx_data = edx_data[['user_id_mapped', 'course_id_mapped', 'viewed']]\n",
    "\n",
    "# Ensure 'viewed' column is binary (0 or 1)\n",
    "edx_data['viewed'] = edx_data['viewed'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train, temp = train_test_split(edx_data, test_size=0.3, random_state=42)\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the train.txt file\n",
    "train_file = 'datasets/edx/train.txt'\n",
    "train[['user_id_mapped', 'course_id_mapped', 'viewed']].to_csv(train_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the val.txt file\n",
    "val_file = 'datasets/edx/val.txt'\n",
    "val[['user_id_mapped', 'course_id_mapped', 'viewed']].to_csv(val_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the test.txt file\n",
    "test_file = 'datasets/edx/test.txt'\n",
    "test[['user_id_mapped', 'course_id_mapped', 'viewed']].to_csv(test_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the text.txt file (course id and name mapping)\n",
    "text_file = 'datasets/edx/text.txt'\n",
    "with open(text_file, 'w') as f:\n",
    "    for idx, name in course_name_map.items():\n",
    "        f.write(f\"{idx} {name}\\n\")\n",
    "\n",
    "print(\"Preprocessing complete. Files saved as train.txt, val.txt, test.txt, and text.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_embeddings.npy file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the course embeddings CSV file\n",
    "course_embeddings_df = pd.read_csv('datasets/edx/course_embeddings.csv')\n",
    "\n",
    "# Extract the embeddings (assuming the first two columns are course_id and course_name)\n",
    "course_embeddings = course_embeddings_df.iloc[:, 2:].values  # Extracting the embeddings as a NumPy array\n",
    "\n",
    "# Save the embeddings as a .npy file\n",
    "np.save('datasets/edx/course_embeddings.npy', course_embeddings)\n",
    "\n",
    "print(\"course_embeddings.npy file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (16, 512)\n",
      "Saved precomputed embeddings to datasets/edx/precomputed_embeddings.npy\n",
      "Saved course info to datasets/edx/course_info.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# อ่านไฟล์ CSV\n",
    "csv_path = 'datasets/edx/course_embeddings.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# แยก course_id และ course_name ออกจาก embeddings\n",
    "course_ids = df['course_id'].values\n",
    "course_names = df['course_name'].values\n",
    "\n",
    "# เลือกเฉพาะคอลัมน์ที่เป็น embeddings\n",
    "embedding_columns = [col for col in df.columns if col.startswith('embed_')]\n",
    "embeddings = df[embedding_columns].values\n",
    "\n",
    "# บันทึก embeddings เป็นไฟล์ .npy\n",
    "output_path = 'datasets/edx/precomputed_embeddings.npy'\n",
    "np.save(output_path, embeddings)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Saved precomputed embeddings to {output_path}\")\n",
    "\n",
    "# บันทึก course_ids และ course_names เป็นไฟล์ text เพื่อเก็บข้อมูลอ้างอิง\n",
    "with open('datasets/edx/course_info.txt', 'w') as f:\n",
    "    for id, name in zip(course_ids, course_names):\n",
    "        f.write(f\"{id}\\t{name}\\n\")\n",
    "\n",
    "print(\"Saved course info to datasets/edx/course_info.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Files saved as train.txt, val.txt, test.txt, and text.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Load the dataset\n",
    "edx_data = pd.read_csv('datasets/edx/HMXPC13_DI_v2_5-14-14.csv')\n",
    "course_embeddings = pd.read_csv('datasets/edx/course_embeddings.csv')\n",
    "\n",
    "# Create a mapping from course_id to a unique integer\n",
    "course_id_map = {cid: idx for idx, cid in enumerate(course_embeddings['course_id'].unique())}\n",
    "course_name_map = {idx: name for idx, name in enumerate(course_embeddings['course_name'])}\n",
    "\n",
    "# Create a mapping from user_id to a unique integer\n",
    "user_id_map = {uid: idx for idx, uid in enumerate(edx_data['userid_DI'].unique())}\n",
    "\n",
    "# Apply the mappings to the data, using a default value for unmapped entries\n",
    "edx_data['user_id_mapped'] = edx_data['userid_DI'].map(user_id_map)\n",
    "edx_data['course_id_mapped'] = edx_data['course_id'].map(course_id_map)\n",
    "\n",
    "# Drop rows with NaN values in the mapped columns (if mapping fails)\n",
    "edx_data.dropna(subset=['user_id_mapped', 'course_id_mapped'], inplace=True)\n",
    "\n",
    "# Only keep relevant columns\n",
    "edx_data = edx_data[['user_id_mapped', 'course_id_mapped', 'viewed']]\n",
    "\n",
    "# Ensure 'viewed' column is binary (0 or 1)\n",
    "edx_data['viewed'] = edx_data['viewed'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Initialize StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data into train and temp sets using StratifiedShuffleSplit\n",
    "for train_index, temp_index in sss.split(edx_data, edx_data['viewed']):\n",
    "    train = edx_data.iloc[train_index]\n",
    "    temp = edx_data.iloc[temp_index]\n",
    "\n",
    "# Split the temp set into validation and test sets using StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for val_index, test_index in sss.split(temp, temp['viewed']):\n",
    "    val = temp.iloc[val_index]\n",
    "    test = temp.iloc[test_index]\n",
    "\n",
    "# Save the train.txt file\n",
    "train_file = 'datasets/edx/train.txt'\n",
    "train[['user_id_mapped', 'course_id_mapped', 'viewed']].to_csv(train_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the val.txt file\n",
    "val_file = 'datasets/edx/val.txt'\n",
    "val[['user_id_mapped', 'course_id_mapped', 'viewed']].to_csv(val_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the test.txt file\n",
    "test_file = 'datasets/edx/test.txt'\n",
    "test[['user_id_mapped', 'course_id_mapped', 'viewed']].to_csv(test_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the text.txt file (course id and name mapping)\n",
    "text_file = 'datasets/edx/text.txt'\n",
    "with open(text_file, 'w') as f:\n",
    "    for idx, name in course_name_map.items():\n",
    "        f.write(f\"{idx} {name}\\n\")\n",
    "\n",
    "print(\"Preprocessing complete. Files saved as train.txt, val.txt, test.txt, and text.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caser_venv_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
