{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\computer15\\desktop\\caser_pytorch\\caser_venv_py39\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\computer15\\desktop\\caser_pytorch\\caser_venv_py39\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.1-cp39-cp39-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/11.0 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/11.0 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.6/11.0 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.0/11.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 11.8 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "usage_report = pd.read_csv(r'C:\\Users\\COMPUTER15\\Desktop\\caser_pytorch\\datasets\\edx\\HMXPC13_DI_v2_5-14-14.csv')\n",
    "embed_df = pd.read_csv(r'C:\\Users\\COMPUTER15\\Desktop\\caser_pytorch\\datasets\\edx\\course_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(usage_report, embed_df, on='course_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Files saved as train.txt, val.txt, test.txt, and text.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "edx_data = pd.read_csv('datasets/edx/HMXPC13_DI_v2_5-14-14.csv')\n",
    "course_embeddings = pd.read_csv('datasets/edx/course_embeddings.csv')\n",
    "\n",
    "# Create a mapping from course_id to a unique integer, starting from 1\n",
    "course_id_map = {cid: idx + 1 for idx, cid in enumerate(course_embeddings['course_id'].unique())}\n",
    "course_name_map = {idx + 1: name for idx, name in enumerate(course_embeddings['course_name'])}\n",
    "\n",
    "# Create a mapping from user_id to a unique integer, starting from 1\n",
    "user_id_map = {uid: idx + 1 for idx, uid in enumerate(edx_data['userid_DI'].unique())}\n",
    "\n",
    "# Apply the mappings to the data, using a default value for unmapped entries\n",
    "edx_data['user_id_mapped'] = edx_data['userid_DI'].map(user_id_map)\n",
    "edx_data['course_id_mapped'] = edx_data['course_id'].map(course_id_map)\n",
    "\n",
    "# Drop rows with NaN values in the mapped columns (if mapping fails)\n",
    "edx_data.dropna(subset=['user_id_mapped', 'course_id_mapped'], inplace=True)\n",
    "\n",
    "# Ensure mapped columns are integers\n",
    "edx_data['user_id_mapped'] = edx_data['user_id_mapped'].astype(int)\n",
    "edx_data['course_id_mapped'] = edx_data['course_id_mapped'].astype(int)\n",
    "\n",
    "# Only keep relevant columns\n",
    "edx_data = edx_data[['user_id_mapped', 'course_id_mapped', 'viewed']]\n",
    "\n",
    "# Ensure 'viewed' column is binary (0 or 1)\n",
    "edx_data['viewed'] = edx_data['viewed'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train, temp = train_test_split(edx_data, test_size=0.3, random_state=42)\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the train.txt file\n",
    "train_file = 'datasets/edx/train.txt'\n",
    "train[['user_id_mapped', 'course_id_mapped', 'viewed']].to_csv(train_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the val.txt file\n",
    "val_file = 'datasets/edx/val.txt'\n",
    "val[['user_id_mapped', 'course_id_mapped', 'viewed']].to_csv(val_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the test.txt file\n",
    "test_file = 'datasets/edx/test.txt'\n",
    "test[['user_id_mapped', 'course_id_mapped', 'viewed']].to_csv(test_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the text.txt file (course id and name mapping)\n",
    "text_file = 'datasets/edx/text.txt'\n",
    "with open(text_file, 'w') as f:\n",
    "    for idx, name in course_name_map.items():\n",
    "        f.write(f\"{idx} {name}\\n\")\n",
    "\n",
    "print(\"Preprocessing complete. Files saved as train.txt, val.txt, test.txt, and text.txt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training instances: 448796\n",
      "Validation instances: 96171\n",
      "Testing instances: 96171\n"
     ]
    }
   ],
   "source": [
    "train_instances = len(train)\n",
    "val_instances = len(val)\n",
    "test_instances = len(test)\n",
    "print(f\"Training instances: {train_instances}\")\n",
    "print(f\"Validation instances: {val_instances}\")\n",
    "print(f\"Testing instances: {test_instances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances after splitting: 641138\n"
     ]
    }
   ],
   "source": [
    "total_split_instances = train_instances + val_instances + test_instances\n",
    "print(f\"Total instances after splitting: {total_split_instances}\")\n",
    "# print(f\"Matches original cleaned dataset: {total_split_instances == cleaned_instances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_embeddings.npy file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the course embeddings CSV file\n",
    "course_embeddings_df = pd.read_csv('datasets/edx/course_embeddings.csv')\n",
    "\n",
    "# Extract the embeddings (assuming the first two columns are course_id and course_name)\n",
    "course_embeddings = course_embeddings_df.iloc[:, 2:].values  # Extracting the embeddings as a NumPy array\n",
    "\n",
    "# Save the embeddings as a .npy file\n",
    "np.save('datasets/edx/course_embeddings.npy', course_embeddings)\n",
    "\n",
    "print(\"course_embeddings.npy file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02198753,  0.01548861,  0.01192251, ..., -0.05242273,\n",
       "         0.04461347,  0.02681613],\n",
       "       [ 0.03154026,  0.01396392,  0.01248035, ..., -0.05908341,\n",
       "        -0.06023016, -0.00614068],\n",
       "       [-0.03563004, -0.03190602, -0.00545556, ...,  0.09224168,\n",
       "        -0.02804756, -0.0601936 ],\n",
       "       ...,\n",
       "       [ 0.02253859, -0.02491898, -0.01404075, ...,  0.06840524,\n",
       "        -0.02014308,  0.01584824],\n",
       "       [-0.06399932, -0.05794781,  0.0367046 , ...,  0.08097213,\n",
       "        -0.11322082, -0.07104141],\n",
       "       [-0.04584137,  0.03171441, -0.03698539, ...,  0.04262418,\n",
       "         0.05728782, -0.02593604]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (16, 512)\n",
      "Saved precomputed embeddings to datasets/edx/precomputed_embeddings.npy\n",
      "Saved course info to datasets/edx/course_info.txt\n",
      "Saved course ID mapping to datasets/edx/course_id_mapping.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# อ่านไฟล์ CSV\n",
    "csv_path = 'datasets/edx/course_embeddings.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# สร้าง mapping ใหม่สำหรับ course_id โดยเริ่มจาก 1\n",
    "course_id_map = {old_id: new_id for new_id, old_id in enumerate(df['course_id'], start=1)}\n",
    "\n",
    "# แยก course_id และ course_name ออกจาก embeddings\n",
    "course_ids = [course_id_map[id] for id in df['course_id']]\n",
    "course_names = df['course_name'].values\n",
    "\n",
    "# เลือกเฉพาะคอลัมน์ที่เป็น embeddings\n",
    "embedding_columns = [col for col in df.columns if col.startswith('embed_')]\n",
    "embeddings = df[embedding_columns].values\n",
    "\n",
    "# บันทึก embeddings เป็นไฟล์ .npy\n",
    "output_path = 'datasets/edx/precomputed_embeddings.npy'\n",
    "np.save(output_path, embeddings)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Saved precomputed embeddings to {output_path}\")\n",
    "\n",
    "# บันทึก course_ids และ course_names เป็นไฟล์ text เพื่อเก็บข้อมูลอ้างอิง\n",
    "with open('datasets/edx/course_info.txt', 'w') as f:\n",
    "    for new_id, name in zip(course_ids, course_names):\n",
    "        f.write(f\"{new_id}\\t{name}\\n\")\n",
    "\n",
    "print(\"Saved course info to datasets/edx/course_info.txt\")\n",
    "\n",
    "# บันทึก mapping ระหว่าง course_id เดิมและใหม่\n",
    "with open('datasets/edx/course_id_mapping.txt', 'w') as f:\n",
    "    for old_id, new_id in course_id_map.items():\n",
    "        f.write(f\"{old_id}\\t{new_id}\\n\")\n",
    "\n",
    "print(\"Saved course ID mapping to datasets/edx/course_id_mapping.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Files saved as train.txt, val.txt, test.txt, and text.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Load the dataset\n",
    "edx_data = pd.read_csv('datasets/edx/HMXPC13_DI_v2_5-14-14.csv')\n",
    "course_embeddings = pd.read_csv('datasets/edx/course_embeddings.csv')\n",
    "\n",
    "# Create a mapping from course_id to a unique integer starting from 1\n",
    "course_id_map = {cid: idx + 1 for idx, cid in enumerate(course_embeddings['course_id'].unique())}\n",
    "course_name_map = {idx + 1: name for idx, name in enumerate(course_embeddings['course_name'])}\n",
    "\n",
    "# Create a mapping from user_id to a unique integer starting from 1\n",
    "user_id_map = {uid: idx + 1 for idx, uid in enumerate(edx_data['userid_DI'].unique())}\n",
    "\n",
    "# Apply the mappings to the data, using a default value for unmapped entries\n",
    "edx_data['user_id_mapped'] = edx_data['userid_DI'].map(user_id_map)\n",
    "edx_data['course_id_mapped'] = edx_data['course_id'].map(course_id_map)\n",
    "\n",
    "# Drop rows with NaN values in the mapped columns (if mapping fails)\n",
    "edx_data.dropna(subset=['user_id_mapped', 'course_id_mapped'], inplace=True)\n",
    "\n",
    "# Ensure mapped columns are integers\n",
    "edx_data['user_id_mapped'] = edx_data['user_id_mapped'].astype(int)\n",
    "edx_data['course_id_mapped'] = edx_data['course_id_mapped'].astype(int)\n",
    "\n",
    "# Only keep relevant columns (remove 'viewed')\n",
    "edx_data = edx_data[['user_id_mapped', 'course_id_mapped']]\n",
    "\n",
    "# Initialize GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data into train and temp sets using GroupShuffleSplit (ensuring users are not split)\n",
    "for train_index, temp_index in gss.split(edx_data, groups=edx_data['user_id_mapped']):\n",
    "    train = edx_data.iloc[train_index]\n",
    "    temp = edx_data.iloc[temp_index]\n",
    "\n",
    "# Split the temp set into validation and test sets using GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for val_index, test_index in gss.split(temp, groups=temp['user_id_mapped']):\n",
    "    val = temp.iloc[val_index]\n",
    "    test = temp.iloc[test_index]\n",
    "\n",
    "# Save the train.txt file\n",
    "train_file = 'datasets/edx/train.txt'\n",
    "train[['user_id_mapped', 'course_id_mapped']].to_csv(train_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the val.txt file\n",
    "val_file = 'datasets/edx/val.txt'\n",
    "val[['user_id_mapped', 'course_id_mapped']].to_csv(val_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the test.txt file\n",
    "test_file = 'datasets/edx/test.txt'\n",
    "test[['user_id_mapped', 'course_id_mapped']].to_csv(test_file, sep=' ', header=False, index=False)\n",
    "\n",
    "# Save the text.txt file (course id and name mapping)\n",
    "text_file = 'datasets/edx/text.txt'\n",
    "with open(text_file, 'w') as f:\n",
    "    for idx, name in course_name_map.items():\n",
    "        f.write(f\"{idx} {name}\\n\")\n",
    "\n",
    "print(\"Preprocessing complete. Files saved as train.txt, val.txt, test.txt, and text.txt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course Relevance Analysis with Embeddings:\n",
      "Number of courses analyzed: 16\n",
      "Average relevance score: 0.22\n",
      "Average similarity score with most similar course: 0.59\n",
      "\n",
      "Top 5 Most Relevant Courses:\n",
      "Course ID: MITx/6.00x/2013_Spring\n",
      "  Relevance Score: 0.31\n",
      "  Most Similar Course: MITx/6.00x/2012_Fall\n",
      "  Similarity Score: 0.95\n",
      "\n",
      "Course ID: MITx/3.091x/2013_Spring\n",
      "  Relevance Score: 0.30\n",
      "  Most Similar Course: MITx/3.091x/2012_Fall\n",
      "  Similarity Score: 0.94\n",
      "\n",
      "Course ID: MITx/2.01x/2013_Spring\n",
      "  Relevance Score: 0.26\n",
      "  Most Similar Course: MITx/3.091x/2012_Fall\n",
      "  Similarity Score: 0.20\n",
      "\n",
      "Course ID: MITx/14.73x/2013_Spring\n",
      "  Relevance Score: 0.24\n",
      "  Most Similar Course: HarvardX/PH278x/2013_Spring\n",
      "  Similarity Score: 0.36\n",
      "\n",
      "Course ID: MITx/8.MReV/2013_Summer\n",
      "  Relevance Score: 0.24\n",
      "  Most Similar Course: MITx/8.02x/2013_Spring\n",
      "  Similarity Score: 0.21\n",
      "\n",
      "Correlation between embedding similarity and relevance score: 0.2197\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# อ่านไฟล์\n",
    "usage_report = pd.read_csv(r'C:\\Users\\COMPUTER15\\Desktop\\caser_pytorch\\datasets\\edx\\HMXPC13_DI_v2_5-14-14.csv')\n",
    "embed_df = pd.read_csv(r'C:\\Users\\COMPUTER15\\Desktop\\caser_pytorch\\datasets\\edx\\course_embeddings.csv')\n",
    "\n",
    "def analyze_course_relevance_with_embeddings(usage_report, embed_df):\n",
    "    # คำนวณอัตราการมีส่วนร่วมสำหรับแต่ละคอร์ส\n",
    "    course_engagement = usage_report.groupby('course_id').agg({\n",
    "        'registered': 'sum',\n",
    "        'viewed': 'sum',\n",
    "        'explored': 'sum',\n",
    "        'certified': 'sum',\n",
    "        'nplay_video': 'mean',\n",
    "        'nchapters': 'mean',\n",
    "        'nforum_posts': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # คำนวณอัตราส่วนของการมีส่วนร่วม\n",
    "    course_engagement['view_rate'] = course_engagement['viewed'] / course_engagement['registered']\n",
    "    course_engagement['explore_rate'] = course_engagement['explored'] / course_engagement['registered']\n",
    "    course_engagement['certification_rate'] = course_engagement['certified'] / course_engagement['registered']\n",
    "\n",
    "    # กำหนดเกณฑ์ความเกี่ยวข้อง\n",
    "    course_engagement['relevance_score'] = (\n",
    "        course_engagement['view_rate'] * 0.3 +\n",
    "        course_engagement['explore_rate'] * 0.4 +\n",
    "        course_engagement['certification_rate'] * 0.3\n",
    "    )\n",
    "\n",
    "    # รวม embeddings เข้ากับข้อมูลการมีส่วนร่วม\n",
    "    embedding_cols = [col for col in embed_df.columns if col.startswith('embed_')]\n",
    "    course_data = pd.merge(course_engagement, embed_df[['course_id'] + embedding_cols], on='course_id')\n",
    "\n",
    "    # คำนวณความคล้ายคลึงระหว่างคอร์สโดยใช้ embeddings\n",
    "    embeddings = course_data[embedding_cols].values\n",
    "    embeddings_normalized = normalize(embeddings)\n",
    "    course_similarities = cosine_similarity(embeddings_normalized)\n",
    "\n",
    "    # หาคอร์สที่คล้ายกันที่สุดสำหรับแต่ละคอร์ส\n",
    "    most_similar_courses = []\n",
    "    for i, course_id in enumerate(course_data['course_id']):\n",
    "        similarities = course_similarities[i]\n",
    "        most_similar_idx = similarities.argsort()[-2]  # -1 คือตัวมันเอง\n",
    "        most_similar_courses.append({\n",
    "            'course_id': course_id,\n",
    "            'most_similar_course': course_data.iloc[most_similar_idx]['course_id'],\n",
    "            'similarity_score': similarities[most_similar_idx]\n",
    "        })\n",
    "\n",
    "    most_similar_df = pd.DataFrame(most_similar_courses)\n",
    "    course_data = pd.merge(course_data, most_similar_df, on='course_id')\n",
    "\n",
    "    print(\"Course Relevance Analysis with Embeddings:\")\n",
    "    print(f\"Number of courses analyzed: {len(course_data)}\")\n",
    "    print(f\"Average relevance score: {course_data['relevance_score'].mean():.2f}\")\n",
    "    print(f\"Average similarity score with most similar course: {course_data['similarity_score'].mean():.2f}\")\n",
    "\n",
    "    # แสดงคอร์สที่มีความเกี่ยวข้องสูงสุด 5 อันดับแรก\n",
    "    top_courses = course_data.nlargest(5, 'relevance_score')\n",
    "    print(\"\\nTop 5 Most Relevant Courses:\")\n",
    "    for _, course in top_courses.iterrows():\n",
    "        print(f\"Course ID: {course['course_id']}\")\n",
    "        print(f\"  Relevance Score: {course['relevance_score']:.2f}\")\n",
    "        print(f\"  Most Similar Course: {course['most_similar_course']}\")\n",
    "        print(f\"  Similarity Score: {course['similarity_score']:.2f}\")\n",
    "        print()\n",
    "\n",
    "    # วิเคราะห์ความสัมพันธ์ระหว่างความคล้ายคลึงของ embeddings และความเกี่ยวข้องของคอร์ส\n",
    "    correlation = course_data[['similarity_score', 'relevance_score']].corr()['relevance_score']['similarity_score']\n",
    "    print(f\"Correlation between embedding similarity and relevance score: {correlation:.4f}\")\n",
    "\n",
    "    return course_data\n",
    "\n",
    "# ทำการวิเคราะห์\n",
    "course_data = analyze_course_relevance_with_embeddings(usage_report, embed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caser_venv_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
